{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"contextual-langdetect","text":"<p>A context-aware language detection library that improves accuracy by considering document-level language patterns.</p>"},{"location":"#use-case","title":"Use Case","text":"<p>This library is designed for processing corpora where individual lines or sentences might be in different languages, but with a strong prior that there are only one or two primary languages. It uses document-level context to improve accuracy in cases where individual sentences might be ambiguously detected.</p> <p>For example, in a primarily Chinese corpus: - Some sentences might be detected at an individual level as Japanese, but if   they don't contain kana characters, they're likely Chinese - Some sentences might be detected as Wu Chinese (wuu), but in a Mandarin   context they're likely Mandarin - The library uses the dominant language(s) in the corpus to resolve these   ambiguities</p> <p>This is particularly useful for: - Transcriptions of bilingual conversations, including - Language instruction texts and transcriptions - Mixed-language documents where the majority language should inform ambiguous   cases</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Accurate language detection with confidence scores</li> <li>Context-aware detection that uses surrounding text to disambiguate</li> <li>Special case handling for commonly confused languages (e.g., Wu Chinese,   Japanese without kana)</li> <li>Support for mixed language documents</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install contextual-langdetect\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>from contextual_langdetect import contextual_detect\n\n# Process a document with context-awareness\nsentences = [\n    \"\u4f60\u597d\u3002\",  # Detected as ZH\n    \"\u4f60\u597d\u5417?\",  # Detected as ZH\n    \"\u5f88\u597d\u3002\",  # Detected as JA when model=small\n    \"\u6211\u5bb6\u4e5f\u6709\u56db\u4e2a,\u521a\u597d\u3002\",  # Detected as ZH\n    \"\u90a3\u4e48\u73b0\u5728\u5929\u6c14\u5f88\u51b7,\u4f60\u8981\u5f00\u6696\u6c14\u5417?\",  # Detected as WUU\n    \"Okay, fine I'll see you next week.\",  # English\n    \"Great, I'll see you then.\",  # English\n]\n\n# Context-unaware language detection\nlanguages = contextual_detect(sentences, context_correction=False)\nprint(languages)\n# Output: ['zh', 'zh', 'ja', 'zh', 'wuu', 'en', 'en']\n\n# Context-aware language detection\nlanguages = contextual_detect(sentences)\nprint(languages)\n# Output: ['zh', 'zh', 'zh', 'zh', 'zh', 'en', 'en']\n\n# Context-aware detection with language biasing\n# Specify expected languages to improve detection in ambiguous cases\nlanguages = contextual_detect(sentences, languages=[\"zh\", \"en\"])\nprint(languages)\n# Output: ['zh', 'zh', 'zh', 'zh', 'zh', 'en', 'en']\n\n# Force a specific language for all sentences\nlanguages = contextual_detect(sentences, languages=[\"en\"])\nprint(languages)\n# Output: ['en', 'en', 'en', 'en', 'en', 'en', 'en']\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"<p>This library builds upon: - fast-langdetect for base   language detection</p>"},{"location":"#development","title":"Development","text":"<p>For development instructions, see DEVELOPMENT.md.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Context-Aware Detection - Learn how the context-aware language detection algorithm works</li> <li>Language Detection Tool - Documentation for the language detection development tool</li> </ul>"},{"location":"#related-projects","title":"Related Projects","text":"<ul> <li>audio2anki - Extract audio from video files for creating Anki language flashcards</li> <li>add2anki - Browser extension to add words and phrases to Anki language learning decks</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#author","title":"Author","text":"<p>Oliver Steele (@osteele on GitHub)</p>"},{"location":"analyze_text_tool/","title":"Text Analysis Tool","text":"<p>The <code>tools/analyze_text.py</code> script is an internal development tool for examining the context-aware language detection algorithm. It helps developers understand how the language detection behaves with and without context.</p> <p>Run it via:</p> <pre><code>just analyze /path/to/data.txt\n</code></pre> <p>or:</p> <pre><code>uv run tools/analyze_text.py /path/to/data.txt\n</code></pre>"},{"location":"analyze_text_tool/#features","title":"Features","text":"<ul> <li>Two-pass language detection:</li> <li>Line-by-line analysis showing individual language detection results</li> <li>Context-aware analysis showing how context affects language detection</li> <li>Shows original file line numbers for easy reference</li> <li>Highlights ambiguous language detections</li> <li>Shows confidence scores for each detection</li> <li>Indicates when context changes the detected language</li> <li>Skips analysis of comments and blank lines (but shows them in output)</li> </ul>"},{"location":"analyze_text_tool/#usage","title":"Usage","text":""},{"location":"analyze_text_tool/#file-analysis","title":"File Analysis","text":"<p>Analyze a text file:</p> <pre><code>python tools/analyze_text.py input.txt\n</code></pre> <p>Example output: <pre><code>Text Analysis Results\n====================\n\nBasic Statistics:\n- Characters: 42\n- Lines: 3\n- Words: 8\n- Paragraphs: 2\n\nCharacter Categories:\n- Letters: 32 (76.2%)\n  - Uppercase: 5\n  - Lowercase: 27\n- Numbers: 2 (4.8%)\n- Punctuation: 4 (9.5%)\n- Whitespace: 4 (9.5%)\n\nUnicode Scripts:\n- Latin: 28 (66.7%)\n- Han: 10 (23.8%)\n- Common: 4 (9.5%)\n\nWord Boundaries:\n- Word breaks: 8\n- Sentence breaks: 2\n- Line breaks: 3\n</code></pre></p>"},{"location":"analyze_text_tool/#interactive-mode","title":"Interactive Mode","text":"<p>Run in interactive mode for quick analysis:</p> <pre><code>python tools/analyze_text.py -i\n</code></pre> <p>Example session: <pre><code>Enter text to analyze (Ctrl+D or Ctrl+C to exit)\n\nText&gt; Hello, \u4e16\u754c!\nCharacter analysis:\n- ASCII: Hello,  (6 chars)\n- CJK: \u4e16\u754c (2 chars)\n- Punctuation: , ! (2 chars)\n- Total: 10 characters\n\nText&gt; \u4f60\u597d\uff0cworld!\nCharacter analysis:\n- ASCII: world (5 chars)\n- CJK: \u4f60\u597d (2 chars)\n- Punctuation: \uff0c ! (2 chars)\n- Total: 9 characters\n</code></pre></p>"},{"location":"analyze_text_tool/#example-output","title":"Example Output","text":"<p>Given a file <code>test.txt</code> with mixed Chinese and English:</p> <pre><code># Test file with mixed languages\n\u4f60\u597d\u3002\nHello, how are you?\n\n# Another section\n\u6211\u5f88\u597d\uff0c\u8c22\u8c22\u3002\n</code></pre> <p>Running the tool produces:</p> <pre><code>Analyzing 3 non-empty, non-comment lines from test.txt\n\n=== LINE-BY-LINE ANALYSIS ===\n\u250f\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503  # \u2503 Text                 \u2503 Language  \u2503 Confidence \u2503 Status    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502  1 \u2502 # Test file with... \u2502          \u2502           \u2502          \u2502\n\u2502  2 \u2502 \u4f60\u597d\u3002              \u2502 ZH       \u2502     0.980 \u2502 OK       \u2502\n\u2502  3 \u2502 Hello, how are you? \u2502 EN       \u2502     0.950 \u2502 OK       \u2502\n\u2502  4 \u2502                     \u2502          \u2502           \u2502          \u2502\n\u2502  5 \u2502 # Another section   \u2502          \u2502           \u2502          \u2502\n\u2502  6 \u2502 \u6211\u5f88\u597d\uff0c\u8c22\u8c22\u3002      \u2502 ZH       \u2502     0.930 \u2502 OK       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n=== CONTEXT-AWARE RESULTS ===\n\u250f\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503  # \u2503 Original  \u2503 Resolved  \u2503 Confidence \u2503 Status    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502  2 \u2502 ZH       \u2502          \u2502     0.980 \u2502 OK       \u2502\n\u2502  3 \u2502 EN       \u2502          \u2502     0.950 \u2502 OK       \u2502\n\u2502  6 \u2502 ZH       \u2502          \u2502     0.930 \u2502 OK       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"analyze_text_tool/#implementation-details","title":"Implementation Details","text":""},{"location":"analyze_text_tool/#line-filtering","title":"Line Filtering","text":"<ul> <li>Lines starting with <code>#</code> are treated as comments</li> <li>Blank lines are skipped for language analysis</li> <li>Both comments and blank lines are shown in the LINE-BY-LINE table</li> <li>Only content lines appear in the CONTEXT-AWARE table</li> </ul>"},{"location":"analyze_text_tool/#language-detection","title":"Language Detection","text":"<p>The tool performs language detection in two passes:</p> <ol> <li>Line-by-Line Analysis:</li> <li>Each non-comment, non-blank line is analyzed independently</li> <li>Shows the most likely language and its confidence score</li> <li> <p>Marks detections as AMBIGUOUS if confidence is low</p> </li> <li> <p>Context-Aware Analysis:</p> </li> <li>Analyzes content lines as a group</li> <li>Uses surrounding text to improve accuracy</li> <li>Shows when context changes the detected language</li> <li>Only includes non-comment, non-blank lines</li> </ol>"},{"location":"analyze_text_tool/#output-format","title":"Output Format","text":"<p>The tool produces two tables:</p> <ol> <li>LINE-BY-LINE ANALYSIS:</li> <li>Shows all lines from the file</li> <li>Includes line numbers for reference</li> <li>Empty cells for comments and blank lines</li> <li> <p>Language, confidence, and status for content lines</p> </li> <li> <p>CONTEXT-AWARE RESULTS:</p> </li> <li>Shows only content lines</li> <li>Original detected language</li> <li>Changes made by context (if any)</li> <li>Confidence scores</li> <li>Detection status</li> </ol>"},{"location":"analyze_text_tool/#dependencies","title":"Dependencies","text":"<ul> <li>Python's built-in `</li> </ul>"},{"location":"context_aware_detection/","title":"Context-Aware Language Detection","text":""},{"location":"context_aware_detection/#overview","title":"Overview","text":"<p>contextual-langdetect uses document-level context to improve language detection accuracy in mixed-language corpora. It's particularly useful when you have a strong prior belief about the number and distribution of languages in your corpus.</p>"},{"location":"context_aware_detection/#how-it-works","title":"How It Works","text":"<ol> <li>First Pass: Each sentence is analyzed independently using fast-langdetect</li> <li>Context Building:</li> <li>Identifies primary languages (those appearing in &gt;10% of sentences)</li> <li>Builds confidence scores for each detected language</li> <li>Ambiguity Resolution:</li> <li>Uses document context to resolve ambiguous cases</li> <li>Applies special case handling for known detection challenges</li> </ol>"},{"location":"context_aware_detection/#example-chinese-text-processing","title":"Example: Chinese Text Processing","text":"<p>Consider a corpus of primarily Chinese text. Individual sentences might be misidentified:</p> <pre><code>sentences = [\n    \"\u4e2d\u56fd\u662f\u4e00\u4e2a\u4f1f\u5927\u7684\u56fd\u5bb6\",  # Clearly Mandarin\n    \"\u4f60\u597d\",                 # Could be detected as zh/ja/wuu\n    \"\u8c22\u8c22\u5927\u5bb6\",             # Could be ambiguous\n    \"\u6211\u5f88\u9ad8\u5174\u89c1\u5230\u4f60\"         # Could be detected as Japanese without kana\n]\n</code></pre> <p>The library will: 1. Detect that Mandarin is the primary language 2. Notice that some sentences are detected as Japanese but lack kana characters 3. Resolve these ambiguous cases in favor of Mandarin</p>"},{"location":"context_aware_detection/#special-cases","title":"Special Cases","text":""},{"location":"context_aware_detection/#japanese-vs-chinese","title":"Japanese vs Chinese","text":"<ul> <li>Japanese text typically contains kana (hiragana/katakana) characters</li> <li>If a sentence is detected as Japanese but contains no kana, in a Chinese context it's likely Chinese</li> </ul>"},{"location":"context_aware_detection/#wu-chinese-wuu","title":"Wu Chinese (wuu)","text":"<ul> <li>Wu Chinese shares many characters with Mandarin</li> <li>In a primarily Mandarin context, sentences detected as Wu are likely Mandarin</li> </ul>"},{"location":"context_aware_detection/#configuration","title":"Configuration","text":"<p>The library uses configurable thresholds: - Primary language threshold: 10% of sentences - Confidence threshold: 0.70 for ambiguity detection - Alternative language probability threshold: 0.30 for considering alternative languages</p>"},{"location":"context_aware_detection/#best-practices","title":"Best Practices","text":"<ol> <li>Document Structure</li> <li>Group related sentences together</li> <li>Process complete documents rather than isolated sentences</li> <li> <p>Keep context boundaries (e.g., paragraphs, sections) intact</p> </li> <li> <p>Language Distribution</p> </li> <li>Works best when there's a clear primary language</li> <li>Handles 1-2 primary languages well</li> <li> <p>May need tuning for documents with many equally represented languages</p> </li> <li> <p>Performance</p> </li> <li>Process documents in batches rather than individual sentences</li> <li>Use <code>contextual_detect()</code> instead of multiple <code>detect_language()</code> calls</li> <li>Consider sentence length when setting confidence thresholds</li> </ol>"},{"location":"context_aware_detection/#limitations-of-per-sentence-detection","title":"Limitations of Per-Sentence Detection","text":"<p>Traditional language detection analyzes each sentence in isolation, which has limitations: - Short phrases may have insufficient linguistic features for reliable detection - Some phrases look similar across related languages (e.g., Chinese, Japanese) - No benefit from the context of surrounding text</p>"},{"location":"context_aware_detection/#two-pass-context-aware-approach","title":"Two-Pass Context-Aware Approach","text":"<p>contextual-langdetect addresses these limitations with a context-aware approach:</p> <ol> <li>First Pass - Independent Analysis:</li> <li>Process each sentence independently with fast-langdetect</li> <li>Classify each detection as \"confident\" or \"ambiguous\" based on confidence score</li> <li>For confident detections (above threshold), trust the detected language</li> <li> <p>For ambiguous detections (below threshold), mark for further processing</p> </li> <li> <p>Document-Level Language Analysis:</p> </li> <li>Identify primary languages from confident detections</li> <li>Find languages that appear with significant frequency (&gt;10% of sentences)</li> <li>Create a statistical model of the document's language distribution</li> <li> <p>Apply a prior assumption that documents typically contain 1-2 primary languages</p> </li> <li> <p>Second Pass - Context Resolution:</p> </li> <li>For ambiguous sentences, use the document context to make better decisions</li> <li>Look at probability distribution across languages</li> <li>Select the most likely primary language based on both sentence-level probabilities and document-level statistics</li> </ol> <p>This mimics how humans use context to understand language - we naturally use surrounding text to help decode ambiguous phrases.</p>"},{"location":"context_aware_detection/#statistical-language-model","title":"Statistical Language Model","text":"<p>The statistical model used for context-aware detection is based on Bayesian principles:</p> <ol> <li>Prior assumption: Documents typically contain 1-2 primary languages</li> <li>This is implemented by focusing on languages that appear in &gt;10% of confident detections</li> <li> <p>Languages with only occasional appearances are considered less likely to be the document's primary language</p> </li> <li> <p>Language distribution analysis:</p> </li> <li>Track frequency of each confidently detected language</li> <li>Calculate language prevalence as percentage of total document</li> <li>Languages that appear frequently (above the 10% threshold) are identified as \"primary languages\"</li> <li> <p>In most cases, this results in 1-2 languages being identified as primary</p> </li> <li> <p>Bayesian-inspired disambiguation:</p> </li> <li>For ambiguous sentences, examine their individual language probability distributions</li> <li>Combine these with the document's language distribution (prior)</li> <li>Select the language that maximizes P(language | sentence) \u00d7 P(language | document)</li> <li>This balances individual sentence evidence with document-level context</li> </ol> <p>This approach is particularly effective for documents with consistent language patterns, such as: - Single-language documents with occasional ambiguous phrases - Alternating language patterns (e.g., bilingual conversations) - Documents with one primary language and occasional phrases in another language</p>"},{"location":"context_aware_detection/#special-case-handling","title":"Special Case Handling","text":"<p>The library includes special case handling for common language detection challenges:</p> <ol> <li>Wu Chinese (wuu) correction</li> <li>Wu Chinese is often detected as a separate language from Mandarin Chinese</li> <li> <p>When Wu is detected with low confidence and Mandarin is a primary language, treat as Mandarin</p> </li> <li> <p>Chinese/Japanese disambiguation</p> </li> <li>Some Chinese sentences are misdetected as Japanese due to shared characters</li> <li>When a sentence is detected as Japanese without any kana characters, and Chinese is a primary language,      treat it as Chinese</li> </ol>"},{"location":"context_aware_detection/#api-usage","title":"API Usage","text":""},{"location":"context_aware_detection/#process-a-document-with-context-awareness","title":"Process a Document with Context Awareness","text":"<pre><code>from contextual_langdetect import contextual_detect\n\n# List of sentences to analyze\nsentences = [\n    \"Hello world\",\n    \"This is English\",\n    \"\u4f60\u597d\u4e16\u754c\",\n    \"\u8fd9\u662f\u4e2d\u6587\",\n    \"Short text\"  # Ambiguous due to length\n]\n\n# Process with context awareness\nlanguages = contextual_detect(sentences)\n\n# Print results\nfor sentence, lang in zip(sentences, languages):\n    print(f\"{sentence}: {lang}\")\n</code></pre>"},{"location":"context_aware_detection/#access-lower-level-apis","title":"Access Lower-Level APIs","text":"<pre><code>from contextual_langdetect import detect_language, get_language_probabilities\n\n# Get detailed detection result\nresult = detect_language(\"Hello world\")\nprint(f\"Language: {result.language}\")\nprint(f\"Confidence: {result.confidence}\")\nprint(f\"Is ambiguous: {result.is_ambiguous}\")\n\n# Get probability distribution\nprobs = get_language_probabilities(\"Hello \u4f60\u597d\")\nprint(probs)  # {'en': 0.6, 'zh': 0.4}\n</code></pre>"},{"location":"context_aware_detection/#benefits-of-context-aware-detection","title":"Benefits of Context-Aware Detection","text":"<ul> <li>Improved accuracy for short phrases and sentences</li> <li>Reduced need for explicit language specification</li> <li>More consistent results for mixed-language documents</li> <li>Mimics human understanding of language in context</li> <li>Graceful degradation when detection is challenging</li> </ul>"},{"location":"context_aware_detection/#limitations","title":"Limitations","text":"<ul> <li>Language detection is probabilistic and may not be perfect</li> <li>Very short sentences (1-2 words) remain challenging</li> <li>Closely related languages can be difficult to distinguish</li> <li>Depends on fast-langdetect's capabilities and limitations</li> </ul>"},{"location":"detect_languages_tool/","title":"Language Detection Tool","text":"<p>The <code>tools/detect_languages.py</code> script is an internal development tool for exploring <code>fast-langdetect</code> language detection behavior. I use it during the development of this package, to:</p> <ul> <li>Understand how the language detection models behave with different inputs</li> <li>Debug cases where language detection might be giving unexpected results</li> <li>Compare the behavior of small (fast) and large (accurate) models</li> <li>Identify potential edge cases or ambiguous text</li> <li>Verify language detection accuracy for different scripts and language combinations</li> </ul> <p>The script provides language detection capabilities using the fast-langdetect library. It can analyze text either from a file or interactively, and can use either a small (fast) or large (more accurate) model.</p> <p>Run it via:</p> <pre><code>just detect /path/to/data.txt\n</code></pre> <p>or:</p> <pre><code>uv run tools/detect_langauges.py /path/to/data.txt\n</code></pre>"},{"location":"detect_languages_tool/#features","title":"Features","text":"<ul> <li>Detect languages in text files or interactive input</li> <li>Compare results between small (fast) and large (accurate) models</li> <li>Display results in a formatted table with highlighted highest scores</li> <li>Handle multiple languages per sentence with confidence scores</li> <li>Skip comments and blank lines in input files</li> </ul>"},{"location":"detect_languages_tool/#usage","title":"Usage","text":""},{"location":"detect_languages_tool/#file-analysis","title":"File Analysis","text":"<p>Analyze a text file using either the small or large model:</p> <pre><code># Using small model (default)\npython tools/detect_languages.py input.txt\n\n# Using large model\npython tools/detect_languages.py input.txt --model=large\n</code></pre> <p>Example output: <pre><code>                             Language Detection Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Text                              \u2503   ZH \u2503   EN \u2503 Other                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 \u4f60\u597d\u3002                             \u2502 0.98 \u2502      \u2502 yue:0.02                 \u2502\n\u2502 Hello, how are you?               \u2502      \u2502 0.95 \u2502 fr:0.03 de:0.02         \u2502\n\u2502 \u6211\u5f88\u597d\uff0c\u8c22\u8c22\u3002                      \u2502 0.93 \u2502      \u2502 yue:0.07                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>The table shows: - The input text in the first column - Columns for languages that appear frequently or with high confidence - An \"Other\" column showing additional detected languages - Bold scores indicate the highest confidence for each line</p>"},{"location":"detect_languages_tool/#interactive-mode","title":"Interactive Mode","text":"<p>Run in interactive mode to compare small and large models side by side:</p> <pre><code>python tools/detect_languages.py -i\n</code></pre> <p>Example session: <pre><code>Enter text to analyze (Ctrl+D or Ctrl+C to exit)\n\nText&gt; \u4f60\u597d\uff0c\u4e16\u754c\uff01\nSmall model: zh:0.98 yue:0.02\nLarge model: zh:0.99 yue:0.01\n\nText&gt; Hello, world!\nSmall model: en:0.95 fr:0.03 de:0.02\nLarge model: en:0.98 fr:0.01 de:0.01\n\nText&gt; Bonjour le monde!\nSmall model: fr:0.92 en:0.05 de:0.03\nLarge model: fr:0.97 en:0.02 de:0.01\n</code></pre></p>"},{"location":"detect_languages_tool/#implementation-details","title":"Implementation Details","text":""},{"location":"detect_languages_tool/#language-selection","title":"Language Selection","text":"<p>The script identifies \"major languages\" for column display based on two criteria: 1. Languages that appear in at least 25% of the sentences with a score \u2265 0.2 2. Languages that have a score at least twice as high as any other language in a sentence</p>"},{"location":"detect_languages_tool/#score-display","title":"Score Display","text":"<ul> <li>Scores below 0.01 are filtered out</li> <li>The highest score for each line is shown in bold</li> <li>Languages are sorted by total score across all sentences for consistent column ordering</li> </ul>"},{"location":"detect_languages_tool/#input-file-format","title":"Input File Format","text":"<ul> <li>Lines starting with <code>#</code> are treated as comments and skipped</li> <li>Blank lines are ignored</li> <li>All other lines are treated as text to analyze</li> </ul>"},{"location":"detect_languages_tool/#dependencies","title":"Dependencies","text":"<ul> <li><code>fast-langdetect</code>: Language detection library</li> <li><code>rich</code>: Terminal formatting and tables</li> </ul>"},{"location":"detect_languages_tool/#error-handling","title":"Error Handling","text":"<ul> <li>Gracefully handles Ctrl+C and Ctrl+D in interactive mode</li> <li>Validates command-line arguments</li> <li>Skips invalid input lines</li> <li>Handles empty detection results</li> </ul>"},{"location":"detect_languages_tool/#development-use-cases","title":"Development Use Cases","text":""},{"location":"detect_languages_tool/#debugging-ambiguous-cases","title":"Debugging Ambiguous Cases","text":"<p>Use the interactive mode to quickly test phrases that might be ambiguous between languages: <pre><code>Text&gt; \u6211\u7cfb\u5b66\u751f\nSmall model: zh:0.45 yue:0.55\nLarge model: yue:0.75 zh:0.25\n</code></pre></p>"},{"location":"detect_languages_tool/#model-comparison","title":"Model Comparison","text":"<p>Compare how the small and large models handle edge cases: <pre><code>Text&gt; Je suis \u00e9tudiant\nSmall model: fr:0.85 en:0.10 de:0.05\nLarge model: fr:0.95 en:0.03 de:0.02\n</code></pre></p>"},{"location":"detect_languages_tool/#batch-analysis","title":"Batch Analysis","text":"<p>Analyze test files containing known problematic or edge cases: <pre><code>python tools/detect_languages.py tests/data/mandarin-wu-ambiguous.txt --model=large\n</code></pre></p>"},{"location":"development/","title":"Development Guide for contextual-langdetect","text":"<p>This document describes the development process and tools for the contextual-langdetect project.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install just - Command runner</li> <li>Install uv - Python package manager</li> </ul>"},{"location":"development/#setup","title":"Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/osteele/contextual-langdetect.git\ncd contextual-langdetect\n\n# Install development dependencies\njust setup\n</code></pre>"},{"location":"development/#development-commands","title":"Development Commands","text":"<p>The project uses <code>just</code> as a command runner. Here are the available commands:</p> <pre><code># Run all checks: format, lint, typecheck, and tests\njust check\n\n# Format code\njust format\n\n# Fix linting issues\njust fix\n\n# Lint code\njust lint\n\n# Type check\njust typecheck\n\n# Run tests (or a specific test with arguments)\njust test [args]\n\n# Run the main CLI\njust run [args]\n</code></pre>"},{"location":"development/#development-tools","title":"Development Tools","text":"<p>The repository includes CLI tools for development and testing purposes. These are not included in the package distribution.</p> <pre><code># Analyze a file with the text analysis tool\njust analyze path/to/textfile.txt [args]\n\n# Generate language statistics from a file\njust detect path/to/textfile.txt [args]\n</code></pre>"},{"location":"development/#tool-documentation","title":"Tool Documentation","text":"<ul> <li>Text Analysis Tool - Detailed documentation for the text analysis tool</li> <li>Language Detection Tool - Documentation for the language detection development tool</li> </ul>"},{"location":"development/#algorithm-documentation","title":"Algorithm Documentation","text":"<ul> <li>Context-Aware Detection - Learn how the context-aware language detection algorithm works</li> </ul>"},{"location":"development/#project-structure","title":"Project Structure","text":"<p>The project follows a standard Python package structure:</p> <pre><code>.\n\u251c\u2500\u2500 contextual_langdetect/  # Main package code\n\u251c\u2500\u2500 docs/                   # Documentation\n\u251c\u2500\u2500 tests/                  # Test suite\n\u251c\u2500\u2500 tools/                  # Development tools\n\u251c\u2500\u2500 pyproject.toml         # Project configuration and dependencies\n\u251c\u2500\u2500 justfile               # Command runner configuration\n\u251c\u2500\u2500 README.md              # User documentation\n\u2514\u2500\u2500 DEVELOPMENT.md         # Developer documentation (this file)\n</code></pre>"},{"location":"development/#dependencies","title":"Dependencies","text":"<p>This project uses <code>pyproject.toml</code> for dependency management. The development dependencies are specified in the <code>[dependency-groups.dev]</code> section.</p> <p>The <code>uv</code> tool is configured to handle dependency management. You don't need to run <code>pip install</code> or <code>uv pip install</code> manually - <code>uv run</code>, <code>uv test</code>, etc. will sync the environment automatically.</p>"}]}